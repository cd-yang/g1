import json
import os
import time

import streamlit as st

from ollama import Client

ollama_client = Client(
    host='http://192.168.100.231:11434',
)

USE_CHINESE_PROMPT = True
INCLUDE_AFSIM_BACKGROUND = True
MAX_STEP_COUNT = 5 # Max steps to prevent infinite thinking time. Can be adjusted.

def make_api_call(messages, max_tokens, is_final_answer=False):
    for attempt in range(3):
        try:
            response = ollama_client.chat(
                model="qwen2.5:72b",
                # model="qwen2.5:32b",
                # model="qwen2.5-coder:32b-instruct-fp16",
                # model="qwq:32b-preview-fp16",
                messages=messages,
                options={"temperature":0.2, 
                        #  "num_predict":max_tokens # è¿™é‡Œå¯èƒ½ä¼šå¯¼è‡´ unicode è§£æé”™è¯¯
                         },
                format='json',
            )
            # print('ollama response:', response)

            # å°è¯•å¤„ç† qwen2.5:72b æ¨¡å‹è¿”å›çš„ content åŒ…å«çš„ unicode å­—ç¬¦é”™è¯¯ï¼Œä½†æ˜¯ä¼¼ä¹æ²¡æœ‰æ•ˆæœï¼Œå…ˆä¸ä½¿ç”¨ qwen2.5:72b æ¨¡å‹
            # response_content = response['message']['content']
            # response_content = response_content.encode('utf-8').decode('unicode_escape')
            # return json.loads(response_content)

            return json.loads(response['message']['content'])
        except Exception as e:
            print('********** Exception start *********', f"Error: {str(e)}\n")
            if attempt == 2:
                if is_final_answer:
                    return {"title": "Error", "content": f"Failed to generate final answer after 3 attempts. Error: {str(e)}"}
                else:
                    return {"title": "Error", "content": f"Failed to generate step after 3 attempts. Error: {str(e)}", "next_action": "final_answer"}
            time.sleep(1)  # Wait for 1 second before retrying

def generate_response(prompt):
    if INCLUDE_AFSIM_BACKGROUND:
        system_info = """AFSIMæ˜¯ä¸€ä¸ªé€šç”¨çš„å»ºæ¨¡æ¡†æ¶ï¼Œç”±ç¾å›½ç©ºå†›ç ”ç©¶å®éªŒå®¤ï¼ˆAFRLï¼‰å¼€å‘å’Œç»´æŠ¤1ã€‚å®ƒçš„ä¸»è¦ç›®çš„æ˜¯ç”¨äºæ¨¡æ‹Ÿå’Œåˆ†æä½œæˆ˜ç¯å¢ƒï¼Œå¸®åŠ©ç”¨æˆ·è¯„ä¼°å†›äº‹æˆ˜ç•¥å’Œæˆ˜æœ¯å†³ç­–çš„æœ‰æ•ˆæ€§ã€‚
å…·ä½“æ¥è¯´ï¼ŒAFSIMæä¾›äº†å®Œæ•´çš„ä»¿çœŸç¯å¢ƒï¼ŒåŒ…æ‹¬ï¼š
1.å„ç§æˆ˜æ–—å¹³å°ï¼ˆä¾‹å¦‚é£æœºã€å¦å…‹ã€èˆ¹åªç­‰ï¼‰çš„æ¨¡æ‹Ÿã€‚
2.å„ç§æ­¦å™¨ç³»ç»Ÿçš„æ¨¡æ‹Ÿã€‚
3.ç¯å¢ƒæ•ˆåº”çš„å»ºæ¨¡ï¼Œä¾‹å¦‚å¤©æ°”ã€åœ°å½¢ç­‰ã€‚

ä½ æ˜¯ä¸€ä½AFSIMå»ºæ¨¡çš„ä¸“å®¶ï¼Œå°†ç”¨æˆ·çš„éœ€æ±‚ï¼Œåˆ†è§£æˆä¸€ä¸ªä¸ªç»†é¢—ç²’åº¦çš„éœ€æ±‚ã€‚å¯ä»¥ä¸€æ­¥ä¸€æ­¥è§£é‡Šä½ çš„æ¨ç†ã€‚"""
    else:
        system_info = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä¸€æ­¥ä¸€æ­¥è§£é‡Šä½ çš„æ¨ç†ã€‚"
    if USE_CHINESE_PROMPT:
        messages = [
            {"role": "system", "content": system_info + """å¯¹äºæ¯ä¸ªæ­¥éª¤ï¼Œæä¾›ä¸€ä¸ªæ ‡é¢˜æ¥æè¿°ä½ åœ¨è¯¥æ­¥éª¤ä¸­æ‰€åšçš„äº‹æƒ…ï¼Œä»¥åŠå†…å®¹ã€‚å†³å®šæ˜¯å¦éœ€è¦å¦ä¸€ä¸ªæ­¥éª¤ï¼Œæˆ–è€…æ˜¯å¦å‡†å¤‡å¥½ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
ä»¥ JSON æ ¼å¼å›å¤"title"ã€"content"å’Œ"next_action"ï¼ˆ"continue"æˆ–"final_answer"ï¼‰é”®ã€‚å°½å¯èƒ½å¤šåœ°ä½¿ç”¨æ¨ç†æ­¥éª¤ã€‚è‡³å°‘ 3 ä¸ªã€‚äº†è§£ä½ ä½œä¸ºLLMçš„å±€é™æ€§ï¼Œä»¥åŠä½ èƒ½åšä»€ä¹ˆå’Œä¸èƒ½åšä»€ä¹ˆã€‚åœ¨ä½ çš„æ¨ç†ä¸­ï¼ŒåŒ…æ‹¬å¯¹æ›¿ä»£ç­”æ¡ˆçš„æ¢ç´¢ã€‚è€ƒè™‘ä½ å¯èƒ½æ˜¯é”™çš„ï¼Œå¦‚æœä½ çš„æ¨ç†æ˜¯é”™çš„ï¼Œå®ƒä¼šåœ¨å“ªé‡Œã€‚å……åˆ†æµ‹è¯•æ‰€æœ‰å…¶ä»–å¯èƒ½æ€§ã€‚ä½ å¯èƒ½ä¼šé”™ã€‚å½“æ‚¨è¯´æ‚¨æ­£åœ¨é‡æ–°æ£€æŸ¥æ—¶ï¼Œè¯·çœŸæ­£é‡æ–°æ£€æŸ¥ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ç§æ–¹æ³•è¿›è¡Œã€‚ä¸è¦åªæ˜¯è¯´æ‚¨æ­£åœ¨é‡æ–°æ£€æŸ¥ã€‚ä½¿ç”¨è‡³å°‘ 3 ç§æ–¹æ³•æ¥å¾—å‡ºç­”æ¡ˆã€‚ä½¿ç”¨æœ€ä½³å®è·µã€‚

æœ‰æ•ˆ JSON å“åº”çš„ç¤ºä¾‹ï¼š
```json
{
"title"ï¼š"è¯†åˆ«å…³é”®ä¿¡æ¯"ï¼Œ
"content"ï¼š"è¦å¼€å§‹è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ä»”ç»†æ£€æŸ¥ç»™å®šçš„ä¿¡æ¯å¹¶ç¡®å®šå°†æŒ‡å¯¼æˆ‘ä»¬è§£å†³è¿‡ç¨‹çš„å…³é”®è¦ç´ ã€‚è¿™æ¶‰åŠ..."ï¼Œ
"next_action"ï¼š"continue"
}```
"""},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": "è°¢è°¢ï¼æˆ‘ç°åœ¨ä¼šæŒ‰ç…§æˆ‘çš„æŒ‡ç¤ºä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼Œåœ¨åˆ†è§£é—®é¢˜ä¹‹åä»å¤´å¼€å§‹ã€‚"}
        ]
    else:
        messages = [
            {"role": "system", "content": """You are an expert AI assistant that explains your reasoning step by step. For each step, provide a title that describes what you're doing in that step, along with the content. Decide if you need another step or if you're ready to give the final answer. Respond in JSON format with 'title', 'content', and 'next_action' (either 'continue' or 'final_answer') keys. USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.

Example of a valid JSON response:
```json
{
    "title": "Identifying Key Information",
    "content": "To begin solving this problem, we need to carefully examine the given information and identify the crucial elements that will guide our solution process. This involves...",
    "next_action": "continue"
}```
"""},
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": "Thank you! I will now think step by step following my instructions, starting at the beginning after decomposing the problem."}
        ]
    
    steps = []
    step_count = 1
    total_thinking_time = 0
    
    while True:
        start_time = time.time()
        step_data = make_api_call(messages, 300)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time

        if 'title' not in step_data:
            step_data['title'] = "..."
        if 'content' not in step_data:
            print(time.time(), 'æœªç”Ÿæˆæœ‰æ•ˆ contentï¼Œé‡è¯•...')  # Retry if no valid content generated
            continue
        steps.append((f"Step {step_count}: {step_data['title']}", step_data['content'], thinking_time))
        
        messages.append({"role": "assistant", "content": json.dumps(step_data)})
        
        if step_data['next_action'] == 'final_answer' or step_count > MAX_STEP_COUNT:
            break
        
        step_count += 1

        # Yield after each step for Streamlit to update
        yield steps, None  # We're not yielding the total time until the end

    # Generate final answer
    if USE_CHINESE_PROMPT:
        messages.append({"role": "user", "content": "è¯·æ ¹æ®ä¸Šè¿°æ¨ç†æä¾›æœ€ç»ˆç­”æ¡ˆ"})
    else:
        messages.append({"role": "user", "content": "Please provide the final answer based on your reasoning above."})
    
    start_time = time.time()
    final_data = make_api_call(messages, 200, is_final_answer=True)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time
    
    steps.append(("Final Answer", final_data['content'], thinking_time))

    yield steps, total_thinking_time

def main():
    st.set_page_config(page_title="y1 prototype", page_icon="ğŸ§ ", layout="wide")
    
    st.title("y1: AFSIM éœ€æ±‚åˆ†è§£ demo")
    
    st.markdown("""
    GitHubCopilotWorkspace-like Requirements Decomposition Demo 
    """)
    
    # Text input for user query
    user_query = st.text_input("è¾“å…¥ä½ çš„ AFSIM åœºæ™¯:", placeholder="e.g., å¦‚ä½•ç”Ÿæˆä¸€ä¸ªç®€å•çš„ 1v1 çº¢è“å¯¹æŠ—åœºæ™¯ï¼Ÿ",
                            #    value="å¦‚ä½•ç”Ÿæˆä¸€ä¸ªç®€å•çš„ 1v1 çº¢è“å¯¹æŠ—åœºæ™¯ï¼Ÿ"
                               )
    
    if user_query:
        st.write("æ­£åœ¨ç”Ÿæˆå›å¤...")
        
        # Create empty elements to hold the generated text and total time
        response_container = st.empty()
        time_container = st.empty()
        
        # Generate and display the response
        for steps, total_thinking_time in generate_response(user_query):
            with response_container.container():
                for i, (title, content, thinking_time) in enumerate(steps):
                    if title.startswith("Final Answer"):
                        st.markdown(f"### {title}")
                        st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
                    else:
                        with st.expander(title, expanded=True):
                            st.markdown(content.replace('\n', '<br>'), unsafe_allow_html=True)
            
            # Only show total time when it's available at the end
            if total_thinking_time is not None:
                time_container.markdown(f"**Total thinking time: {total_thinking_time:.2f} seconds**")

if __name__ == "__main__":
    main()
